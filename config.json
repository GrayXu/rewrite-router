{
    "BACKEND_URL": "http://localhost:3000",
    // Route requests based on context token length
    "ROUTING_RULES": {
        "gpt-4.1-auto": {
            "models": {
                "8192": "gpt-4.1-8k",
                "131072": "gpt-4.1"
            },
            "threshold": 0.65
        },
        "gpt-4o-auto": {
            "models": {
                "8192": "gpt-4o-8k",
                "131072": "gpt-4o-2024-08-06"
            },
            "threshold": 0.65
        },
        "gpt-4o-mini-auto": {
            "models": {
                "8192": "gpt-4o-mini-8k",
                "131072": "gpt-4o-mini"
            },
            "threshold": 0.65
        },
        "Qwen/Qwen2.5-72B-Instruct-auto": {
            "models": {
                "32768": "Qwen/Qwen2.5-72B-Instruct",
                "131072": "Qwen/Qwen2.5-72B-Instruct-128K"
            },
            "threshold": 0.7
        }
    },
    // Rewrite params in requests
    "REWRITE_RULES": {
        // rewirte max_tokens
        "yi-lightning": {
            "max_tokens": 5000
        },
        // change model name for cursor
        "cursor-model": {
            "model": "gemini-2.5-pro"
        },
        // Add derived models with different reasoning effort.
        "o3-mini-high": {
            "model": "o3-mini",
            "reasoning_effort": "high"
        },
        "o4-mini-high": {
            "model": "o4-mini",
            "reasoning_effort": "high"
        },
        // Add thinking for claude 3.7 thinking
        "claude-3-7-sonnet-20250219-thinking": {
            "model": "claude-3-7-sonnet-20250219",
            "thinking": {
                "type": "enabled",
                "budget_tokens": 8192
            }
        },
        // Add thinking for gemini-2.5-flash
        "gemini-2.5-flash": {
            "model": "gemini-2.5-flash",
            "reasoning": {
              "max_tokens": 0
            }
        },
        "gemini-2.5-flash-thinking": {
            "model": "gemini-2.5-flash",
            "reasoning": {
              "max_tokens": 16000
            }
        },
        // Force rewrite to use recommended inference parameters.
        "Qwen/QwQ-32B": {
            "min_p": 0.0,
            "top_p": 0.95,
            "top_k": 40,
            "frequency_penalty": 1.0
        },
        // Add tool using
        "gemini-2.5-pro-tool": {
            "model": "gemini-2.5-pro",
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "codeExecution"
                    }
                }
            ]
        },
        "gemini-2.5-pro-search": {
            "model": "gemini-2.5-pro",
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "googleSearch"
                    }
                }
            ]
        },
        "gemini-2.0-pro-search": {
            "model": "gemini-2.0-pro",
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "googleSearch"
                    }
                }
            ]
        },
        "gemini-2.0-flash-search": {
            "model": "gemini-2.0-flash",
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "googleSearch"
                    }
                }
            ]
        },
        "gemini-2.0-flash-thinking-search": {
            "model": "gemini-2.0-flash-thinking",
            "tools": [
                {
                    "type": "function",
                    "function": {
                        "name": "googleSearch"
                    }
                }
            ]
        }
        // ,
        // // insert prompts
        // "prompt-insert-model": {
        //     "message": [
        //         {
        //             "role": "system",
        //             "content": "You are ChatGPT."
        //         }
        //     ]
        // }
    }
}